@article{Portigal2013,
author = {Portigal, Steve},
isbn = {9781933820118},
journal = {Rosenfeld Media},
mendeley-groups = {Usability and interaction},
title = {{Interviewing Users: How to Uncover Compelling Insights}},
year = {2013}
}

@article{Runeson2009,
abstract = {Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors' own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Runeson, Per and H{\"{o}}st, Martin},
doi = {10.1007/s10664-008-9102-8},
eprint = {9809069v1},
file = {:C$\backslash$:/Users/tskripnikova/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Runeson, H{\"{o}}st - 2009 - Guidelines for conducting and reporting case study research in software engineering.pdf:pdf},
issn = {13823256},
journal = {Empirical Software Engineering},
keywords = {Case study,Checklists,Guidelines,Research methodology},
month = {4},
number = {2},
pages = {131--164},
pmid = {28843849},
primaryClass = {arXiv:gr-qc},
publisher = {Springer US},
title = {{Guidelines for conducting and reporting case study research in software engineering}},
url = {http://link.springer.com/10.1007/s10664-008-9102-8},
volume = {14},
year = {2009}
}

@book{Lethbridge2005,
abstract = {Software engineering is an intensively people-oriented activity, yet too little is known about how designers, maintainers, requirements analysts and all other types of software engineers perform their work. In order to improve software engineering tools and practice, it is therefore essential to conduct field studies, i.e. to study real practitioners as they solve real problems. To do so effectively, however, requires an understanding of the techniques most suited to each type of field study task. In this paper, we provide a taxonomy of techniques, focusing on those for data collection. The taxonomy is organized according to the degree of human intervention each requires. For each technique, we provide examples from the literature, an analysis of some of its advantages and disadvantages, and a discussion of how to use it effectively. We also briefly talk about field study design in general, and data analysis.},
author = {Lethbridge, Timothy C. and Sim, Susan Elliott and Singer, Janice},
booktitle = {Empirical Software Engineering},
doi = {10.1007/s10664-005-1290-x},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/Studying{\_}Software{\_}Engineers{\_}Data{\_}Collection{\_}Techni.pdf:pdf},
isbn = {1066400512},
issn = {13823256},
keywords = {Empirical software engineering,Field studies,Work practices},
number = {3},
pages = {311--341},
title = {{Studying software engineers: Data collection techniques for software field studies}},
volume = {10},
year = {2005}
}

@article{Nielsen2003,
abstract = {For 11 studies, we find that the detection of usability problems as a function of number of users tested or heuristic evaluators employed is well modeled as a Poisson process. The model can be used to plan the amount of evaluation required to achieve desired levels of thoroughness or benefits. Results of early tests can provide estimates of the number of problems left to be found and the number of additional evaluations needed to find a given fraction. With quantitative evaluation costs and detection values, the model can estimate the numbers of evaluations at which optimal cost/benefit ratios are obtained and at which marginal utility vanishes. For a “medium” example, we estimate that 16 evaluations would be worth their cost, with maximum benefit/cost ratio at four.},
author = {Nielsen, Jakob and Landauer, Thomas K.},
doi = {10.1145/169059.169166},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/p206-Five Users nielsen.pdf:pdf},
isbn = {0897915755},
issn = {0-89791-575-5},
keywords = {Bacia do,Estratigrafia,Formacao irati,Hidrocarbonetos,Paran{\'{a}}},
number = {[77 f.]},
pages = {206--213},
pmid = {16812074},
title = {{A mathematical model of the finding of usability problems}},
year = {2003}
}

@article{Brooke1996,
abstract = {Usability does not exist in any absolute sense; it can only be defined with reference to particular contexts. This, in turn, means that there are no absolute measures of usability, since, if the usability of an artefact is defined by the context in which that artefact is used, measures of usability must of necessity be defined by that context too. Despite this, there is a need for broad general measures which can be used to compare usability across a range of contexts. In addition, there is a need for " quick and dirty " methods to allow low cost assessments of usability in industrial systems evaluation. This chapter describes the System Usability Scale (SUS) a reliable, low-cost usability scale that can be used for global assessments of systems usability.},
author = {Brooke, John},
doi = {10.1002/hbm.20701},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/sus.pdf:pdf},
isbn = {0748404600},
issn = {1097-0193},
journal = {Usability Evaluation in Industry},
mendeley-groups = {Usability and interaction},
pages = {4--7},
pmid = {19172650},
title = {{SUS - A quick and dirty usability scale}},
year = {1996}
}

@article{Shneiderman1996,
abstract = {A useful starting point for designing advanced graphical user interfaces is the Visual Information-Seeking Mantra: overview first, zoom and filter, then details on demand. But this is only a starting point in trying to understand the rich and varied set of information visualizations that have been proposed in recent years. This paper offers a task by data type taxonomy with seven data types (one-, two-, three-dimensional data, temporal and multi-dimensional data, and tree and network data) and seven tasks (overview, zoom, filter, details-on-demand, relate, history, and extracts).},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Shneiderman, B},
doi = {10.1109/VL.1996.545307},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/Shneiderman1996eyes.pdf:pdf},
isbn = {0-8186-7508-X},
issn = {1049-2615},
pages = {336--343},
pmid = {4986861},
title = {{The eyes have it: a task by data type taxonomy for information visualizations. Proceedings of IEEE Symposium on Visual Languages}},
url = {http://portal.acm.org/citation.cfm?id=832277.834354},
year = {1996}
}

@article{Simon2006,
abstract = {Elaborates a comprehensive theory of human problem solving. The book is divided into 5 parts: The 1st presents foundations of the information processing approach; 3 parts contain detailed analyses of problem solving behavior in specific task areas (cryptarithmetic, logic, and chess); and the last presents the theory. (101/2 p. ref.) (PsycINFO Database Record (c) 2003 APA},
archivePrefix = {arXiv},
arxivId = {arXiv:0910.0734v1},
author = {Simon, Herbert A. and Newell, Allen},
doi = {10.1037/h0030806},
eprint = {arXiv:0910.0734v1},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/82b07ac84aaf30b502c93076cec2accbfcaa.pdf:pdf},
isbn = {0134454030},
issn = {0003-066X},
journal = {American Psychologist},
number = {2},
pages = {145--159},
pmid = {176},
title = {{Human problem solving: The state of the theory in 1970.}},
volume = {26},
year = {2006}
}

@article{Ericsson1980,
abstract = {Behavior and experience are organized around the enjoyment and pursuit of incentives. During the time that an incentive is behaviorally salient, an organism is especially responsive to incentive-related cues. This sustained sensitivity requires postulating a continuing state (denoted by a construct, current concern) with a definite onset (commitment) and offset (consummation or disengagement). Disengagement follows frustration, accompanies the behavioral process of extinction, and involves an incentive-disengagement cycle of invigoration, aggression, depression, and recovery. Depression is thus a normal part of disengagement that may be either adaptive or maladaptive for the individual but is probably adaptive for the species. The theory offers implications for motivation; etiology, symptomatology, and treatment of depression; drug use; and other social problem areas.},
author = {Ericsson, K. Anders and Simon, Herbert A.},
doi = {10.1037/0033-295X.87.3.215},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/Ericsson-Simon80.pdf:pdf},
isbn = {0262550237},
issn = {0033295X},
journal = {Psychological Review},
keywords = {,theoretical {\&} methodological considerations, verbal reports as experimental data, implications for information processing model},
number = {3},
pages = {215--251},
title = {{Verbal reports as data}},
url = {http://s3.amazonaws.com/academia.edu.documents/37092743/Ericsson-Simon80.pdf?AWSAccessKeyId=AKIAJ56TQJRTWSMTNPEA{\&}Expires=1475089614{\&}Signature=2UVSlQ8f3qRxRJXYi0mzspm3C4s{\%}3D{\&}response-content-disposition=inline{\%}3B filename{\%}3DEricsson-Simon80.pdf},
volume = {87},
year = {1980}
}

@book{ericsson1984protocol,
  title={Protocol analysis: Verbal reports as data.},
  author={Ericsson, K Anders and Simon, Herbert A},
  year={1984},
  publisher={the MIT Press}
}

@article{Boren2000,
abstract = {Thinking-aloud protocols may be the most widely used method in usability testing, but the descriptions of this practice in the usability literature and the work habits of practitioners do not conform to the theoretical basis most often cited for it: Ericsson and Simon's seminal work PROTOCOL ANALYSIS: VERBAL REPORTS AS DATA [1]. After reviewing Ericsson and Simon's theoretical basis for thinking aloud, we review the ways in which actual usability practice diverges from this model. We then explore the concept of SPEECH GENRE as an alternative theoretical framework. We first consider uses of this new framework that are consistent with Simon and Ericsson's goal of eliciting a verbal report that is as undirected, undisturbed, and constant as possible. We then go on to consider how the proposed new approach might handle problems that arise in usability testing that appear to require interventions not supported in the older model.},
author = {Boren, M. Ted and Ramey, Judith},
doi = {10.1109/47.867942},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/Thinking{\_}aloud{\_}Reconciling{\_}theory{\_}and{\_}practice.pdf:pdf},
issn = {03611434},
journal = {IEEE Transactions on Professional Communication},
number = {3},
pages = {261--278},
title = {{Thinking aloud: Reconciling theory and practice}},
volume = {43},
year = {2000}
}

@article{Krahmer2004,
abstract = { We report on an exploratory experimental comparison of two different thinking aloud approaches in a usability test that focused on navigation problems in a highly nonstandard Web site. One approach is a rigid application of Ericsson and Simon's (for original paper see Protocol Analysis: Verbal Reports as Data, MIT Press (1993)) procedure. The other is derived from Boren and Ramey's (for original paper see ibid., vol. 43, no. 3, p. 261-278 (2000)) proposal based on speech communication. The latter approach differs from the former in that the experimenter has more room for acknowledging (mm-hmm) contributions from subjects and has the possibility of asking for clarifications and offering encouragement. Comparing the verbal reports obtained with these two methods, we find that the process of thinking aloud while carrying out tasks is not affected by the type of approach that was used. The task performance does differ. More tasks were completed in the B and R condition, and subjects were less lost. Nevertheless, subjects' evaluations of the Web site quality did not differ, nor did the number of different navigation problems that were detected.},
author = {Krahmer, Emiel and Ummelen, Nicole},
doi = {10.1109/TPC.2004.828205},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/01303808.pdf:pdf},
issn = {03611434},
journal = {IEEE Transactions on Professional Communication},
keywords = {Lostness,Thinking aloud,Usability research,Usability testing},
mendeley-groups = {Visual patent exploration},
number = {2},
pages = {105--117},
title = {{Thinking about thinking aloud: A comparison of two verbal protocols for usability testing}},
volume = {47},
year = {2004}
}

@article{Tullis2004,
abstract = {Five questionnaires for assessing the usability of a website were compared in a study with 123 participants. The questionnaires studied were SUS, QUIS, CSUQ, a variant of Microsofts Product Reaction Cards, and one that we have used in our Usability Lab for several years. Each participant performed two tasks on each of two websites: finance.yahoo.com and kiplinger.com. All five questionnaires revealed that one site was significantly preferred over the other. The data were analyzed to determine what the results would have been at different sample sizes from 6 to 14. At a sample size of 6, only 30-40{\%} of the samples would have identified that one of the sites was significantly preferred. Most of the data reach an apparent asymptote at a sample size of 12, where two of the questionnaires (SUS and CSUQ) yielded the same conclusion as the full dataset at least 90{\%} of the time.},
author = {Tullis, Thomas S. and Stetson, Jacqueline N.},
doi = {10.1080/09500782.2014.944427},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/A{\_}Comparison{\_}of{\_}Questionnaires{\_}for{\_}Assessing{\_}Websi.pdf:pdf},
isbn = {9783037852422},
issn = {0950-0782},
journal = {Usability Professional Association Conference},
number = {June},
pages = {1--12},
title = {{A comparison of questionnaires for assessing website usability}},
url = {http://home.comcast.net/{~}tomtullis/publications/UPA2004TullisStetson.pdf},
year = {2004}
}

@misc{Laubheimer2018,
abstract = {Post-test questionnaires like the SUS measure perceived usability of an entire system; post-task scales suggest of problematic parts of a design},
author = {Laubheimer, Page},
title = {{Beyond the NPS: Measuring Perceived Usability with the SUS, NASA-TLX, and the Single Ease Question After Tasks and Usability Tests}},
url = {https://www.nngroup.com/articles/measuring-perceived-usability/},
urldate = {2019-03-26},
year = {2018}
}

@article{Lewis1993,
abstract = {This paper describes recent research in subjective usability measurement at IBM. The focus of the research was the application of psychometric methods to the development and evaluation of questionnaires that measure user satisfaction with system usability. The primary goals of this paper are to (1) discuss the psychometric characteristics of four IBM questionnaires that measure user satisfaction with computer system usability, and (2) provide the questionnaires, with administration and scoring instructions. Usability practitioners can use these questionnaires with confidence to help them measure users' satisfaction with the usability of computer systems.},
author = {Lewis, James R.},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/10.1.1.584.6610.pdf:pdf},
journal = {IBM Technical Report},
number = {1},
pages = {786},
title = {{IBM Computer Usability Satisfaction Questionnaires: Psychometric Evaluation and Instructions for Use. Boca Raton, FL: Human Factors Group}},
volume = {54},
year = {1993}
}

@article{Chin2003,
abstract = {This study is a part of a research effort to develop the Questionnaire for User Interface Satisfaction (QUIS). Participants, 150 PC user group members, rated familiar software products. Two pairs of software categories were compared: 1) software that was liked and disliked, and 2) a standard command line system (CLS) and a menu driven application (MDA). The reliability of the questionnaire was high, Cronbach's alpha=.94. The overall reaction ratings yielded significantly higher ratings for liked software and MDA over disliked software and a CLS, respectively. Frequent and sophisticated PC users rated MDA more satisfying, powerful and flexible than CLS. Future applications of the QUIS on computers are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Chin, J. P. and Diehl, V. A. and Norman, L. K.},
doi = {10.1145/57167.57203},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/chin1988.pdf:pdf},
isbn = {0201142376},
issn = {1098-6596},
number = {January},
pages = {213--218},
pmid = {25246403},
title = {{Development of an instrument measuring user satisfaction of the human-computer interface}},
year = {2003}
}

@article{Johnson,
abstract = {In this paper, we present a prototype for an online interac-tive visualization tool for analyzing the semantic proximities of US patent documents that are related to cancer treatments. This tool allows the user to perform keyword searches and then presents visualizations of sets of relevant patent docu-ments clustered by semantic similarity. Semantic similarity is calculated using a combination of word embeddings obtained using the skip-gram algorithm and the t-SNE dimensional-ity reduction algorithm. The user may then select individual points in the cluster to view more detailed patent informa-tion. This process allows the user to explore the connections between related patents and see more general trends in the semantic shape of the technological space. It is our hope that this tool may serve as one of the starting points for data anal-ysis leading to future innovative approaches to cancer treat-ment.},
author = {Johnson, Daniel K N and Whitehead, Matthew},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/15517-68652-1-PB.pdf:pdf},
journal = {Proceedings of the Thirtieth International Florida Artificial Intelligence Research Society Conference},
keywords = {Special Track on Applications of Artificial Intell},
pages = {235--238},
title = {{A Tool for Visualizing and Exploring Relationships among Cancer-Related Patents}},
url = {http://cs.coloradocollege.edu/{~}mwhitehead/CancerMoonshot/documents/iaai.pdf},
year = {2017}
}

@incollection{Card1999,
abstract = {SaijaCard, S. K., Mackinlay, J. D., and Shneiderman, B. (1999). Using Vision to Think, chapter 1: Information Visualization, pages 1–34. Morgan Kaufmann. http://infovis.fh-potsdam.de/readings/Card1999.pdf {\ldots}},
author = {Card, Stuart K. and Mackinlay, Jock D. and Shneiderman, Ben},
booktitle = {Readings in information visualization: using vision to think},
publisher = {Morgan Kaufmann},
title = {{Readings in information visualization: using vision to think}},
year = {1999}
}

@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/1310.4546.pdf:pdf},
issn = {10495258},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {http://arxiv.org/abs/1310.4546},
year = {2013}
}

@TECHREPORT{Modjeska97,
    author = {David Modjeska},
    title = {Navigation in Electronic Worlds: Research Review for Depth Oral Exam},
    institution ={Toronto, Computer Systems Research Group, University of Toronto},
    year = {1997}
}

@inproceedings{Baudisch2002,
abstract = {Users working with documents that are too large and detailed to fit on the user's screen (e.g. chip designs) have the choice between zooming or applying appropriate visualization techniques. In this paper, we present a comparison of three such techniques. The first, focus plus context screens, are wall-size low-resolution displays with an embedded high-resolution display region. This technique is compared with overview plus detail and zooming/panning. We interviewed fourteen visual surveillance and design professionals from different areas (graphic design, chip design, air traffic control, etc.) in order to create a repre sentative sample of tasks to be used in two experimental comparison studies. In the first experiment, subjects using focus plus context screens to extract information from large static documents completed the two experimental tasks on average 21{\%} and 36{\%} faster than when they used the other interfaces. In the second experiment, focus plus context screens allowed subjects to reduce their error rate in a driving simulation to less than one third of the error rate of the competing overview plus detail setup},
author = {Baudisch, Patrick and Good, Nathaniel and Bellotti, Victoria and Schraedley, Pamela},
booktitle = {CHI '02 Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
doi = {10.1145/503376.503423},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/2002-Baudisch-CHI02-KeepingThingsInContext.pdf:pdf},
isbn = {1581134533},
keywords = {20,5,besides static documents,information systems,micro-macro,modifies them,or detailed illustrations,readings,that remain,there are also,unchanged unless the user},
mendeley-groups = {Usability and interaction},
number = {4},
pages = {259--266},
title = {{Keeping things in context: a comparative evaluation of focus plus context screens, overviews, and zooming}},
url = {http://dl.acm.org/citation.cfm?id=503423},
year = {2002}
}

@article{Skupin2013,
abstract = {BACKGROUND: We implement a high-resolution visualization of the medical knowledge domain using the self-organizing map (SOM) method, based on a corpus of over two million publications. While self-organizing maps have been used for document visualization for some time, (1) little is known about how to deal with truly large document collections in conjunction with a large number of SOM neurons, (2) post-training geometric and semiotic transformations of the SOM tend to be limited, and (3) no user studies have been conducted with domain experts to validate the utility and readability of the resulting visualizations. Our study makes key contributions to all of these issues.$\backslash$n$\backslash$nMETHODOLOGY: Documents extracted from Medline and Scopus are analyzed on the basis of indexer-assigned MeSH terms. Initial dimensionality is reduced to include only the top 10{\%} most frequent terms and the resulting document vectors are then used to train a large SOM consisting of over 75,000 neurons. The resulting two-dimensional model of the high-dimensional input space is then transformed into a large-format map by using geographic information system (GIS) techniques and cartographic design principles. This map is then annotated and evaluated by ten experts stemming from the biomedical and other domains.$\backslash$n$\backslash$nCONCLUSIONS: Study results demonstrate that it is possible to transform a very large document corpus into a map that is visually engaging and conceptually stimulating to subject experts from both inside and outside of the particular knowledge domain. The challenges of dealing with a truly large corpus come to the fore and require embracing parallelization and use of supercomputing resources to solve otherwise intractable computational tasks. Among the envisaged future efforts are the creation of a highly interactive interface and the elaboration of the notion of this map of medicine acting as a base map, onto which other knowledge artifacts could be overlaid.},
author = {Skupin, Andr{\'{e}} and Biberstine, Joseph R. and B{\"{o}}rner, Katy},
doi = {10.1371/journal.pone.0058779},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/journal.pone.0058779.PDF:PDF},
issn = {19326203},
journal = {PLoS ONE},
mendeley-groups = {Visual patent exploration},
number = {3},
title = {{Visualizing the Topical Structure of the Medical Sciences: A Self-Organizing Map Approach}},
volume = {8},
year = {2013}
}

@article{LaurensvanderMaaten2008,
abstract = {We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence theway in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza- tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
archivePrefix = {arXiv},
arxivId = {1307.1662},
author = {{Laurens van der Maaten} and Hinton, Geoffrey},
doi = {10.1007/s10479-011-0841-3},
eprint = {1307.1662},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/vandermaaten08a.pdf:pdf},
issn = {02624079},
journal = {Journal of Machine Learning Research},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {2579--2605},
pmid = {20652508},
title = {{Visualizing Data using t-SNE Laurens}},
volume = {9},
year = {2008}
}

@article{wattenberg2016how,
  author = {Wattenberg, Martin and Viégas, Fernanda and Johnson, Ian},
  title = {How to Use t-SNE Effectively},
  journal = {Distill},
  year = {2016},
  url = {http://distill.pub/2016/misread-tsne},
  doi = {10.23915/distill.00002}
}

@article{hearst1999modern,
  title={Modern Information Retrieval, chapter 10. User Interfaces and Visualization},
  author={Hearst, Marti A},
  journal={Addison Wesley Longman},
  year={1999},
  url = {http://people.ischool.berkeley.edu/~hearst/irbook/10/node3.html#SECTION00122000000000000000}
}

@article{Chen2005,
abstract = {The top 10 unsolved problems list described in this article is a revised and extended version of information visualization problems. These problems are not necessarily imposed by technical barriers, rather, they are problems that might hinder the growth of information visualization as a field. The first three problems highlight issues from a user-centered perspective. The fifth, sixth, and seventh problems are technical challenges in nature. The last three are the ones that need tackling at the disciplinary level. The author broadly defines information visualization as visual representations of the semantics, or meaning, of information. In contrast to scientific visualization, information visualization typically deals with nonnumeric, nonspatial, and high-dimensional data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Chen, Chaomei},
doi = {10.1109/MCG.2005.91},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/2006150005.pdf:pdf},
isbn = {0272-1716},
issn = {02721716},
journal = {IEEE Computer Graphics and Applications},
number = {4},
pages = {12--16},
pmid = {16060568},
title = {{Top 10 unsolved information visualization problems}},
volume = {25},
year = {2005}
}

@article{Carpendale2008,
abstract = {Information visualization research is becoming more established, and as a result, it is becoming increasingly important that research in this field is validated. With the general increase in information visualization research there has also been an increase, albeit disproportionately small, in the amount of empirical work directly focused on information visualization. The purpose of this chapter is to increase awareness of empirical research in general, of its relationship to information visualization in particular; to emphasize its importance; and to encourage thoughtful application of a greater variety of evaluative research methodologies in information visualization.},
author = {Carpendale, Sheelagh},
doi = {10.1007/978-3-540-70956-5_2},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/read{\_}EvaluatingInfoVis.pdf:pdf},
isbn = {354070955X},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {19--45},
pmid = {29263318},
title = {{Evaluating information visualizations}},
volume = {4950 LNCS},
year = {2008}
}

@misc{Alammar2019,
author = {Alammar, Jay},
month = {mar},
title = {{The Illustrated Word2vec}},
url = {https://jalammar.github.io/illustrated-word2vec/},
urldate = {2019-04-03},
year = {2019}
}

@article{Latour2012,
abstract = {In this paper we argue that the new availability of digital data sets allows one to revisit Gabriel Tarde's (1843-1904) social theory that entirely dispensed with using notions such as individual or society. Our argument is that when it was impossible, cumbersome or simply slow to assemble and to navigate through the masses of information on particular items, it made sense to treat data about social connections by defining two levels: one for the element, the other for the aggregates. But once we have the experience of following individuals through their connections (which is often the case with profiles) it might be more rewarding to begin navigating datasets without making the distinction between the level of individual component and that of aggregated structure. It becomes possible to give some credibility to Tarde's strange notion of 'monads'. We claim that it is just this sort of navigational practice that is now made possible by digitally available databases and that such a practice could modify social theory if we could visualize this new type of exploration in a coherent way.},
author = {Latour, Bruno and Jensen, Pablo and Venturini, Tommaso and Grauwin, S{\'{e}}bastian and Boullier, Dominique},
doi = {10.1111/j.1468-4446.2012.01428.x},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/123-MONADS-BJSpdf.pdf:pdf},
issn = {00071315},
journal = {British Journal of Sociology},
keywords = {Actor-network theory,Data visualization,Digital methods,Gabriel Tarde,Social theory},
number = {4},
pages = {590--615},
title = {{'The whole is always smaller than its parts' - a digital test of Gabriel Tardes' monads}},
volume = {63},
year = {2012}
}

@misc{Ribecca2019,
abstract = {The Data Visualisation Catalogue is a project developed by Severino Ribecca to create a library of different information visualisation types. Originally, this project was a way for me to develop my own knowledge of data visualisation and create a reference tool for me to use in the future for my own work. However, I felt it would also be beneficial to both designers and also anyone in a field that requires the use of data visualisation. Each visualisation method was added bit-by-bit, as I individually researched each method, to find the best way to explain how it works and what it is best suited for. Most of the data visualised in the website's example images is dummy data. If you have noticed any mistakes or broken links then feel free to message me on the suggestions page or contact me via the button below.},
author = {Ribecca, Severino},
booktitle = {datavizcatalogue.com},
title = {{Sunburst Diagram - Learn about this chart and tools to create it}},
url = {https://datavizcatalogue.com/methods/sunburst{\_}diagram.html},
urldate = {2019-03-28},
year = {2019}
}

@inproceedings{Wittenburg2015,
abstract = {Patent landscaping is a significant activity for modern businesses. Understanding and taking action on information gleaned from analyses of intellectual property (IP) in specific product or technology domains is necessary for many business decisions. Examples include deci- sions about future product development, about how to guard against litigation threats from competitors, about how to set R{\&}D priorities, about how to value IP for sale or licens- ing, and about how to target companies for mergers and acquisitions. This paper proposes a novel method for visualizing patent landscapes that supports the complex hierarchical, multi-dimensional, multi-typed data found in this domain. Our solution directly addresses the problem of how to provide high-level overviews of such a complex domain while at the same time providing enough detail to draw attention to the most significant areas of difference for further drill-down leading to actionable intelligence.},
address = {Chicago},
annote = {Patent analysis use cases listed},
author = {Wittenburg, Kent and Pekhteryev, Georgiy},
booktitle = {IEEE VIS Workshop 2015 BusinessVis15},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/TR2015-123.pdf:pdf},
mendeley-groups = {Visual patent exploration},
title = {{Multi-Dimensional Comparative Visualization for Patent Landscaping}},
year = {2015}
}

@misc{Strayer2018,
author = {Strayer, Nick},
title = {{Physics based t-SNE}},
url = {https://observablehq.com/@nstrayer/physics-based-t-sne},
urldate = {2019-04-04},
year = {2018}
}

@InProceedings{Banchs2014,
author="Banchs, Rafael E.",
editor="Jaafar, Azizah
and Mohamad Ali, Nazlena
and Mohd Noah, Shahrul Azman
and Smeaton, Alan F.
and Bruza, Peter
and Bakar, Zainab Abu
and Jamil, Nursuriati
and Sembok, Tengku Mohd Tengku",
title="A Comparative Evaluation of 2D And 3D Visual Exploration of Document Search Results",
booktitle="Information Retrieval Technology",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="100--111",
abstract="This work presents and experimental comparison between 2D and 3D search and visualization platforms. The main objective of the study is two explore the following two research questions: what method is most robust in terms of the success rate? And, what method is faster in terms of average search time? The obtained results show that, although successful rates and subject preferences are higher for 3D search and visualization, search times are still lower for 2D search and visualization.",
isbn="978-3-319-12844-3"
}

@misc{Nielsen1998,
author = {Nielsen, Jakob},
title = {{2D is Better Than 3D}},
url = {https://www.nngroup.com/articles/2d-is-better-than-3d/},
urldate = {2019-04-04},
year = {1998}
}

@article{Fabrikant2007,
abstract = {Abstract. Information visualizations have become popular tools for extracting knowledge from large bodies of information. Very little is known on the usability of such 'visual knowledge tools' for information access. The goal of this paper is to show the usability of the spatial metaphor 'scale' to access a large semantic document space. An experiment was conducted to examine whether different user groups can associate graphical changes in resolution in spatialized views with changes of level of detail in an index hierarchy of a digital document collection. Test participants were asked to utilize zoom tools to explore a spatialized subset of the GeoRef database, an extensive collection of geology and earth sciences documents. The outcomes of the experiment suggest that people are able to associate graphical changes in resolution of spatialized views (zooms) with changes in levels of detail of a document collection (hierarchical order). These results are independent of user group membership, but for some displays it takes people longer to make a decision.},
author = {Fabrikant, Sara Irina},
doi = {10.1007/3-540-45424-1_11},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/10.1.1.17.4060.pdf:pdf},
keywords = {scale,semantic spaces,spatial metaphors,spatialization,usability},
pages = {156--172},
title = {{Evaluating the Usability of the Scale Metaphor for Querying Semantic Spaces}},
year = {2007}
}

@inproceedings{djorgovski2018immersive,
  title={Immersive and Collaborative Data Visualization and Analytics Using Virtual Reality},
  author={Djorgovski, S George and Donalek, Ciro and Lombeyda, Santiago and Davidoff, Scott and Amori, Michael},
  booktitle={AGU Fall Meeting Abstracts},
  year={2018}
}

@inproceedings{Hadjar2018,
address = {New York, New York, USA},
author = {Hadjar, Hayet and Meziane, Abdelkrim and Gherbi, Rachid and Setitra, Insaf and Aouaa, Noureddine},
booktitle = {Proceedings of the 2nd International Conference on Web Studies  - WS.2 2018},
doi = {10.1145/3240431.3240442},
isbn = {9781450364386},
keywords = {Data Science,Dataviz,Health data,Interactive visualization,Open data,Virtual Reality,WebVR},
pages = {56--63},
publisher = {ACM Press},
title = {{WebVR based Interactive Visualization of Open Health Data}},
url = {http://dl.acm.org/citation.cfm?doid=3240431.3240442},
year = {2018}
}

@article{Westerman2000,
abstract = {This paper reports two studies investigating the computer-based representation of the semantic information content of databases using object location in two- and three-dimensional virtual space. In the first study, the cognitive demands associated with performing an information search task were examined under conditions where the `goodness of fit' of the spatial-semantic `mapping' was manipulated. The effects of individual differences in spatial ability and associative memory ability also were considered. Results indicated that performance equivalence, between two- and three-dimensional interfaces, could be achieved when the two-dimensional interface accounted for between 50 and 70{\%} of the semantic variance accounted for by the three-dimensional solution. A second study, in which automatic text analysis was used to generate two- and three-dimensional solutions for document sets of varying sizes and types, supported the conclusion that, for the purpose of information search, the amount of additional semantic information that can be conveyed by a three-dimensional solution does not outweigh the associated additional cognitive demands.},
author = {Westerman, S. J. and Cribbin, T.},
doi = {10.1006/ijhc.2000.0417},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/1-s2.0-S1071581900904178-main.pdf:pdf},
issn = {10715819},
journal = {International Journal of Human Computer Studies},
number = {5},
pages = {765--787},
title = {{Mapping semantic information in virtual space: Dimensions, variance and individual differences}},
volume = {53},
year = {2000}
}

@article{Bangor2009,
abstract = {The System Usability Scale (SUS) is an inexpensive, yet effective tool for assessing the usability of a product, including Web sites, cell phones, interactive voice response systems, TV applications, and more. It provides an easy-to-understand score from 0 (negative) to 100 (positive). While a 100-point scale is intuitive in many respects and allows for relative judgments, information describing how the numeric score translates into an absolute judgment of usability is not known. To help answer that question, a seven-point adjective-anchored Likert scale was added as an eleventh question to nearly 1,000 SUS surveys. Results show that the Likert scale scores correlate extremely well with the SUS scores (r=0.822). The addition of the adjective rating scale to the SUS may help practitioners interpret individual SUS scores and aid in explaining the results to non-human factors professionals.},
author = {Bangor, Aaron and Kortum, Philip and Miller, James},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/JUS{\_}Bangor{\_}May2009.pdf:pdf},
journal = {Journal of Usability Studies},
keywords = {SUS,Surveys,System Usability Scale,Usability,User Satisfaction},
number = {3},
pages = {114--123},
title = {{Determining What Individual SUS Scores Mean: Adding an Adjective Rating Scale}},
url = {http://uxpajournal.org/wp-content/uploads/sites/8/pdf/JUS{\_}Bangor{\_}May2009.pdf},
volume = {4},
year = {2009}
}

@article{Treisman1985,
abstract = {Visual analysis appears to be functionally divided between an early preattentive level of processing at which simple features are coded spatially in parallel and a later stage at which focused attention is required to conjoin the separate features into coherent objects. Evidence supporting this dichotomy comes from behavioral studies of visual search, from differences in the ease of texture segregation, from reports of illusory conjunctions when attention is overloaded, from subjects' ability to identify simple features correctly even when they mislocate them, and from the substantial benefit of pre-cuing the location of a relevant item when the task requires that features be conjoined but not when simple features are sufficient. Some further studies of search have revealed a striking asymmetry between several pairs of stimuli which differ in the presence or absence of a single part or property. The asymmetry depends solely on which of the pair is allocated the role of target and which is replicated to form the background items. It suggests that search for the presence of a visual primitive is automatic and parallel, whereas search for the absence of the same feature is serial and requires focused attention. The search asymmetry can be used as an additional diagnostic to help define the functional features extracted by the visual system. {\textcopyright} 1985 Academic Press, Inc. All rights reserved.},
author = {Treisman, Anne},
doi = {10.1016/S0734-189X(85)80004-9},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/1-s2.0-S0734189X85800049-main.pdf:pdf},
issn = {0734189X},
journal = {Computer Vision, Graphics and Image Processing},
number = {2},
pages = {156--177},
title = {{Preattentive processing in vision}},
volume = {31},
year = {1985}
}

@article{Cotropia2013,
abstract = {Patent law both imposes a duty on patent applicants to submit relevant prior art to the PTO and assumes that examiners use this information to determine an application's patentability. In this paper, we examine the validity of these assumptions by studying the use made of applicant-submitted prior art by delving into the actual prosecution process in over a thousand different cases. We find that patent examiners rarely use applicant-submitted art in their rejections to narrow patents, relying almost exclusively on prior art they find themselves. Our findings have implications for a number of important legal and policy disputes, including initiatives to improve patent quality and the strong presumption of validity the law grants issued patents - a presumption that makes patents more difficult to challenge in court.},
author = {Cotropia, Christopher A. and Lemley, Mark A. and Sampat, Bhaven},
doi = {10.1016/j.respol.2013.01.003},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/1-s2.0-S0048733313000085-main.pdf:pdf},
issn = {00487333},
journal = {Research Policy},
keywords = {Bibliometrics,Citations,Patent examination,Patents},
number = {4},
pages = {844--854},
publisher = {Elsevier B.V.},
title = {{Do applicant patent citations matter?}},
url = {http://dx.doi.org/10.1016/j.respol.2013.01.003},
volume = {42},
year = {2013}
}

@book{levene2011introduction,
  title={An introduction to search engines and web navigation},
  author={Levene, Mark},
  year={2011},
  publisher={John Wiley \& Sons}
}

@book{ware2004information,
  title={Information Visualization: Perception for Design},
  author={Ware, C.},
  isbn={9780080478494},
  series={Interactive Technologies},
  url={https://books.google.de/books?id=ZmG\_FiqqyqgC},
  year={2004},
  publisher={Elsevier Science}
}

@article{Stasko2000,
abstract = {A variety of information visualization tools have been developed recently, but relatively little effort has been made to evaluate the effectiveness and utility of the tools. This article describes results from two empirical studies of two visualization tools for depicting hierarchies, in particular, computer file and directory structures. The two tools examined implement space-filling methodologies, one rectangular, the Treemap method, and one circular, the Sunburst method. Participants performed typical file/directory search and analysis tasks using the two tools. In general, performance trends favored the Sunburst tool with respect to correct task performance, particularly on initial use. Performance with Treemap tended to improve over time and use, suggesting a greater learning cost that was partially recouped over time. Each tool afforded somewhat different search strategies, which also appeared to influence performance. Finally, participants strongly preferred the Sunburst tool, citing better ability to convey structure and hierarchy.},
author = {Stasko, John and Catrambone, Richard and Guzdial, Mark and Mcdonald, Kevin},
doi = {10.1006/ijhc.2000.0420},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/ijhcs00.pdf:pdf},
issn = {10715819},
journal = {International Journal of Human Computer Studies},
number = {5},
pages = {663--694},
title = {{Evaluation of space-filling information visualizations for depicting hierarchical structures}},
volume = {53},
year = {2000}
}

@book{bird2009natural,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}

@article{sokal,
author = {R. Sokal, Robert and Rohlf, F},
year = {1962},
month = {02},
pages = {33-40},
title = {Sokal RR, Rohlf FJ. The comparison of dendrograms by objective methods. Taxon 11: 33-40},
volume = {11},
journal = {Taxon},
doi = {10.2307/1217208}
}

@misc{wordnet,
title = {About Wordnet},
url = {https://wordnet.princeton.edu/},
urldate = {2019-05-04},
year = {2010},
publisher={Princeton University}
}

@article{robson2002real,
  title={{Real world research 2nd edition: A resource for social scientists and practitioner-researchers}},
  author={Robson, Colin},
  journal={{Malden: BLACKWELL Publishing}},
  year={2002}
}

@misc{Nielsen2000,
author = {Nielsen, Jakob},
title = {{Why You Only Need to Test with 5 Users}},
url = {https://www.nngroup.com/articles/why-you-only-need-to-test-with-5-users/},
urldate = {2019-05-12},
year = {2000}
}

@misc{rug,
title = {{Rug plots in the margins — geom{\_}rug • ggplot2}},
url = {https://ggplot2.tidyverse.org/reference/geom{\_}rug.html},
urldate = {2019-05-12}
}

@misc{Johnson2018,
author = {Johnson, Ian},
title = {d3 workshop - bl.ocks.org},
url = {http://bl.ocks.org/enjalot/6641917},
urldate = {2019-05-12},
year = {2018}
}

@book{WorldIntellectualPropertyOrganizationWIPO2017,
abstract = {THE IMPACT OF THE ECONOMIC CRISIS AND RECOVERY ON INNOVATION},
archivePrefix = {arXiv},
arxivId = {31},
author = {{World Intellectual Property Organization (WIPO)}},
booktitle = {WIPO Economics and Statistics Series},
doi = {10.1016/0172-2190(79)90016-4},
eprint = {31},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/wipo{\_}pub{\_}941{\_}2017.pdf:pdf},
isbn = {9789280521528},
issn = {01722190},
pages = {60--97},
pmid = {44515704},
title = {{World Intellectual Property Indicators 2017}},
url = {http://www.wipo.int/export/sites/www/freepublications/en/intproperty/941/wipo{\_}pub{\_}941{\_}2013.pdf},
year = {2017}
}

@article{Shneiderman1992,
 author = {Shneiderman, Ben},
 title = {Tree Visualization with Tree-maps: 2-d Space-filling Approach},
 journal = {ACM Trans. Graph.},
 issue_date = {Jan. 1992},
 volume = {11},
 number = {1},
 month = jan,
 year = {1992},
 issn = {0730-0301},
 pages = {92--99},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/102377.115768},
 doi = {10.1145/102377.115768},
 acmid = {115768},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@misc{Takayuki2016,
author = {Takayuki, Ito},
title = {{Treemap in v4 - bl.ocks.org}},
url = {https://bl.ocks.org/ganezasan/52fced34d2182483995f0ca3960fe228},
urldate = {2019-05-12},
year = {2016}
}

@misc{Asturiano,
author = {Asturiano, Vasco},
booktitle = {2019},
title = {{Zoomable Sunburst with Labels - bl.ocks.org}},
url = {https://bl.ocks.org/vasturiano/12da9071095fbd4df434e60d52d2d58d},
urldate = {2019-05-12}
}

@misc{Trott,
author = {Trott, Eduard},
booktitle = {2019},
title = {{Zoomable Sunburst on d3.js v4 - bl.ocks.org}},
url = {https://bl.ocks.org/maybelinot/5552606564ef37b5de7e47ed2b7dc099},
urldate = {2019-05-12}
}

@misc{Gube2019,
abstract = {On websites that have a lot of pages, breadcrumb navigation can greatly enhance the way users find their way around. In terms of usability, breadcrumbs reduce the number of actions a website visitor needs to take in order to get to a higher-level page, and they improve the findability of website sections and pages. They are also an effective visual aid that indicates the location of the user within the website's hierarchy, making it a great source of contextual information for landing pages.},
author = {Gube, Jacob},
booktitle = {Smashing Magazine},
title = {{Breadcrumbs In Web Design: Examples And Best Practices — Smashing Magazine}},
url = {https://www.smashingmagazine.com/2009/03/breadcrumbs-in-web-design-examples-and-best-practices/},
urldate = {2019-05-12},
year = {2019}
}

@misc{breadcrumb,
title = {{BreadCrumb Control for IOS 9 (Swift) for iOS - Cocoa Controls}},
url = {https://www.cocoacontrols.com/controls/breadcrumb-control-for-ios-9-swift},
urldate = {2019-05-12}
}

@inproceedings{Nielsen1994,
 author = {Nielsen, Jakob},
 title = {Enhancing the Explanatory Power of Usability Heuristics},
 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
 series = {CHI '94},
 year = {1994},
 isbn = {0-89791-650-6},
 location = {Boston, Massachusetts, USA},
 pages = {152--158},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/191666.191729},
 doi = {10.1145/191666.191729},
 acmid = {191729},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {heuristic evaluation, usability problems},
} 

@misc{ipc,
booktitle = {World Intellectual Property Organization},
title = {{IPC Publication}},
url = {https://www.wipo.int/classifications/ipc/ipcpub/},
urldate = {2019-05-12},
year = {2018}
}

@misc{IanWetherbee2017,
author = {Wetherbee, Ian},
booktitle = {Google cloud platform},
mendeley-groups = {Visual patent exploration},
title = {{Google Patents Public Datasets: connecting public, paid, and private patent data | Google Cloud Blog}},
url = {https://cloud.google.com/blog/products/gcp/google-patents-public-datasets-connecting-public-paid-and-private-patent-data},
urldate = {2018-11-20},
year = {2017}
}

@article{Abood2018a,
abstract = {{\textcopyright} 2018, The Author(s). Patent landscaping is the process of finding patents related to a particular topic. It is important for companies, investors, governments, and academics seeking to gauge innovation and assess risk. However, there is no broadly recognized best approach to landscaping. Frequently, patent landscaping is a bespoke human-driven process that relies heavily on complex queries over bibliographic patent databases. In this paper, we present Automated Patent Landscaping, an approach that jointly leverages human domain expertise, heuristics based on patent metadata, and machine learning to generate high-quality patent landscapes with minimal effort. In particular, this paper describes a flexible automated methodology to construct a patent landscape for a topic based on an initial seed set of patents. This approach takes human-selected seed patents that are representative of a topic, such as operating systems, and uses structure inherent in patent data such as references and class codes to “expand” the seed set to a set of “probably-related” patents and anti-seed “probably-unrelated” patents. The expanded set of patents is then pruned with a semi-supervised machine learning model trained on seed and anti-seed patents. This removes patents from the expanded set that are unrelated to the topic and ensures a comprehensive and accurate landscape.},
author = {Abood, Aaron and Feltenberger, Dave},
doi = {10.1007/s10506-018-9222-4},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/AutomatedPatentLandscaping{\_}2018Update.pdf:pdf},
issn = {15728382},
journal = {Artificial Intelligence and Law},
keywords = {Classification,Patent landscape,Semi-supervised machine learning,Text analytics},
mendeley-groups = {Visual patent exploration},
number = {2},
pages = {103--125},
publisher = {Springer Netherlands},
title = {{Automated patent landscaping}},
url = {https://doi.org/10.1007/s10506-018-9222-4},
volume = {26},
year = {2018}
}

@article{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Quoc V. and Mikolov, Tomas},
eprint = {1405.4053},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/paragraph{\_}vector.pdf:pdf},
mendeley-groups = {Visual patent exploration},
month = {may},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
year = {2014}
}

@techreport{Trippe2015,
abstract = {With contributions from WIPO Secretariat 2015 2 Chapter 1: Executive Summary Patent Landscape Reports (PLRs) support informed decision-making, and are designed to efficiently address the concerns associated with making high stakes decisions in various areas of technology, increasing the related degree of confidence. For many years decision-makers operated based on personal networks and intuition. With the institution of patent analytics, and PLRs, it is possible for these critical decisions to be made with data-driven, evidence-based approaches that deliver informed choices, and mitigate the associated to the decision risks. The insight gained from the preparation of a patent landscape report can be applied to almost any organization engaged in the evaluation of technology, and its impact on society. Government agencies, as well as private enterprise can gain valuable perspective on a developing, or well-established field by generating a PLR. As an example, PLRs can be used as instruments to inform public policy makers in strategic decisions to related to R{\&}D investment, prioritization, technology transfer or local manufacturing. Patent information can and is increasingly being used as a tool to inform public policy: Policymakers who deal with innovation have increasingly focused on the patent system. They look for clearer, more accessible and geographically more representative information to support key policy processes. They seek a stronger empirical basis for their assessments on the role and impact of the patent system in relation to key areas. While PLR are undoubtedly useful instruments for informed decision-making, producing one can be a time-intensive and expensive process. An organization willing to devote the resources necessary to generate a PLR often does so when they are preparing to make a significant monetary or headcount investment in developing or moving into a technology area. It is critically important to make certain that a PLR is prepared properly in order to ensure that the insight it provides is accurate, and directed towards the key issues associated with technological implementation.},
author = {Trippe, Anthony},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/wipo{\_}pub{\_}946.pdf:pdf},
institution = {World Intellectual Property Organization},
keywords = {answers,ask,black,concept,conceptual,direction,discovery,enquire,enquiry,how,information,intelligence,interrogate,interrogation,investigate,investigation,know,knowledge,learn,problem,questioning,questions,research,sign,signpost,solution,solve,understanding,what,when,where,white,who,why,words},
mendeley-groups = {Visual patent exploration},
title = {{Guidelines for Preparing Patent Landscape Reports}},
url = {http://www.wipo.int/tisc/en/},
year = {2015}
}

@book{Ruotsalainen2008,
abstract = {Approximately 80{\%} of scientific and technical information can be found from patent documents alone, according to a study carried out by the European Patent Office. Patents are also a unique source of information since they are collected, screened and published according to internationally agreed standards. In addition to being an extremely valuable source of technology intelligence, patent documents offer a business competitive intelligence by revealing a competitor's strengths and strategies. Information gained from patents can also help in locating partners for cross-licensing and collaboration. Since the patent system was established, more than 60 million patent applications have been published. It would be impossible to find and analyze relevant documents manually. The need for analysis and evaluation tools for patents has been acknowledged by many solution providers. New solutions are continuously coming onto the market; tools for reading and evaluating individual patents and tools for analyzing sets of patent documents. Solutions of the latter type can still be roughly divided into two groups: tools for retrieving and preparing basic statistics for patent documents, and tools for visualization and progressive analysis of patents. The former group deals only with data in a structured form, whereas the latter also analyzes unstructured text and other data. In this study, four efficient tools for analyzing patent documents were tested: Thomson Reuter's Aureka and Thomson Data Analyzer, Biowisdom's OmniViz, and STN's STN AnaVist. All four tools analyze structured and unstructured data alike. They all visualize the results achieved from clustering the text fields of patent documents and either provide basic statistics graphs themselves or contain filters for performing them with other solutions. The tools were tested with two cases, evaluating their ability to offer technology and business intelligence from patent documents for companies' daily business. Being aware of the state of the art of relevant technology areas is crucial for a company's innovation process. Knowledge of developed techniques and products forestalls overlapping R{\&}D projects and thereby prevents unnecessary investment. Equally important is the recognition of other actors operating in the field. Benchmarking and evaluating a competitor's R{\&}D and market strategies aids in managing one's own processes and locating possible parties for collaboration or cross-licensing. Copyright {\textcopyright} VTT 2008.},
author = {Ruotsalainen, Laura},
booktitle = {VTT Tiedotteita - Valtion Teknillinen Tutkimuskeskus},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/VTTDataMiningTools.pdf:pdf;:C$\backslash$:/Users/tskripnikova/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruotsalainen - 2008 - Data mining tools for technology and competitive intelligence.pdf:pdf},
isbn = {9789513872410},
issn = {12350605},
keywords = {Competitive intelligence,Data mining,Patent data,Patent mapping,Patent mining,Technology intelligence,Text mining,Visualization},
mendeley-groups = {Existing patent {\&} paper visualizations},
number = {2451},
pages = {1--64},
title = {{Data mining tools for technology and competitive intelligence}},
year = {2008}
}

@techreport{report_lenses,
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/Tech Insight Report - Contact Lens.pdf:pdf},
institution = {Gridlogics Technologies Pvt Ltd},
mendeley-groups = {Patent reports},
pages = {5},
title = {{Contact Lenses: Technology Insight Report}},
year = {2014}
}

@techreport{report_3d,
abstract = {IMECE Presentation},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/Tech Insight Report - 3D Printing.pdf:pdf},
institution = {Gridlogics Technologies Pvt Ltd},
mendeley-groups = {Patent reports},
pages = {4},
title = {{3D Printing: Technology Insight Report}},
year = {2014}
}

@misc{python,
publisher = {Python Software Foundation},
title = {{Python}},
url = {https://www.python.org/},
year = {2019}
}

@misc{tensorflow,
publisher = {Google Brain Team},
title = {{TensorFlow}},
url = {https://www.tensorflow.org/}
}

@misc{keras,
publisher = {Fran{\c{c}}ois Chollet},
title = {{Keras}},
url = {https://keras.io/}
}

@misc{numpy,
publisher = {NumPy Developers},
title = {{NumPy}},
url = {https://www.numpy.org/index.html},
year = {2019}
}

@misc{scipy,
publisher = {SciPy developers},
title = {{SciPy}},
url = {https://www.scipy.org/},
year = {2019}
}

@misc{jupyterlab,
publisher = {Project Jupyter},
title = {{JupyterLab}},
url = {https://jupyterlab.readthedocs.io/en/stable/},
year = {2018}
}

@misc{ipython,
publisher = {IPython development team},
title = {{IPython}},
url = {https://ipython.org/},
year = {2019}
}

@misc{axiis,
publisher = {The Axiis Team},
title = {{Axiis : Data Visualization Framework}},
url = {http://www.axiis.org/},
year = {2009}
}

@misc{bokeh,
publisher = {Anaconda and Bokeh Contributors},
title = {{Bokeh}},
url = {https://bokeh.pydata.org/en/latest/},
year = {2018}
}

@misc{Bostock2019,
author = {Bostock, Mike},
title = {{D3.js - Data-Driven Documents}},
url = {https://d3js.org/},
year = {2019}
}

@misc{altair,
publisher = {Altair Developers},
title = {{Altair: Declarative Visualization in Python}},
url = {https://altair-viz.github.io/},
year = {2018}
}

@article{Lui2012,
abstract = {We present langid.py, an off-the-shelf language identiﬁcation tool. We discuss the design and implementation of langid.py, and provide an empirical comparison on 5 longdocument datasets, and 2 datasets from the microblog domain. We ﬁnd that langid.py maintains consistently high accuracy across all domains, making it ideal for end-users that require language identiﬁcation without wanting to invest in preparation of in-domain training data.},
author = {Lui, Marco and Baldwin, Timothy},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/P12-3005.pdf:pdf},
journal = {Proceedings ofthe 50th Annual Meeting ofthe Association for Computational Linguistics},
number = {July},
pages = {25--30},
title = {{langid.py: An off-the-shelf language identification tool}},
url = {https://github.com/saffsd/langid.py},
year = {2012}
}

@article{Lui2011,
abstract = {We show that transductive (cross-domain) learning is an important consideration in building a general-purpose language identiﬁcation system, and develop a feature selection method that generalizes across domains. Our results demonstrate that our method provides improvements in transductive transfer learning for language identiﬁcation. We provide an implementation of the method and show that our system is faster than popular standalone language identiﬁcation systems, while maintaining competitive accuracy.},
author = {Lui, Marco and Baldwin, Timothy},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/I11-1062.pdf:pdf},
journal = {Proceedings ofthe 5th International Joint Conference on Natural Language Processing},
keywords = {baldwin,language identification,marco lui and timothy,ss-domain feature selection for},
number = {1967},
pages = {553--561},
title = {{Cross-domain feature selection for language identification}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.308.4653},
year = {2011}
}

@misc{Cohen2011,
author = {Cohen, Adam},
title = {{FuzzyWuzzy: Fuzzy String Matching in Python - ChairNerd}},
url = {https://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/},
urldate = {2019-05-20},
year = {2011}
}

@ARTICLE{levenshtein,
   author = {{Levenshtein}, V.~I.},
    title = "{Binary Codes Capable of Correcting Deletions, Insertions and Reversals}",
  journal = {Soviet Physics Doklady},
     year = 1966,
    month = feb,
   volume = 10,
    pages = {707},
   adsurl = {https://ui.adsabs.harvard.edu/abs/1966SPhD...10..707L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Zhao2015,
abstract = {This paper reports our submissions to seman-tic textual similarity task, i.e., task 2 in Se-mantic Evaluation 2015. We built our sys-tems using various traditional features, such as string-based, corpus-based and syntactic simi-larity metrics, as well as novel similarity mea-sures based on distributed word representa-tions, which were trained using deep learning paradigms. Since the training and test datasets consist of instances collected from various do-mains, three different strategies of the usage of training datasets were explored: (1) use all available training datasets and build a unified supervised model for all test datasets; (2) se-lect the most similar training dataset and sep-arately construct a individual model for each test set; (3) adopt multi-task learning frame-work to make full use of available training set-s. Results on the test datasets show that using all datasets as training set achieves the best av-eraged performance and our best system ranks 15 out of 73.},
author = {Zhao, Jiang and Lan, Man and Tian, Jun Feng},
doi = {10.18653/v1/s15-2021},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/9cbb598c76e591411e00ff201b4f15efb6fc.pdf:pdf},
number = {SemEval},
pages = {117--122},
title = {{ECNU: Using Traditional Similarity Measurements and Word Embedding for Semantic Textual Similarity Estimation}},
year = {2015}
}

@article{NAGOUDI2018,
abstract = {This article describes our proposed system named LIM-LIG. This system is designed for SemEval 2017 Task1: Semantic Textual Similarity (Track1). LIM-LIG proposes an innovative enhancement to word embedding-based model devoted to measure the semantic similarity in Arabic sentences. The main idea is to exploit the word representations as vectors in a multidimensional space to capture the semantic and syntactic properties of words. IDF weighting and Part-of-Speech tagging are applied on the examined sentences to support the identification of words that are highly descriptive in each sentence. LIM-LIG system achieves a Pearson{\'{s}} correlation of 0.74633, ranking 2nd among all participants in the Arabic monolingual pairs STS task organized within the SemEval 2017 evaluation campaign},
author = {NAGOUDI, El Moatez Billah and Ferrero, J{\'{e}}r{\'{e}}my and Schwab, Didier},
doi = {10.18653/v1/s17-2017},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/63{\_}Paper.pdf:pdf},
number = {August},
pages = {134--138},
title = {{LIM-LIG at SemEval-2017 Task1: Enhancing the Semantic Similarity for Arabic Sentences with Vectors Weighting}},
year = {2018}
}

@article{Arora2017,
abstract = {The success of neural network methods for computing word embeddings has mo-tivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR'16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embed-dings and basic linear regression. The method of Wieting et al. requires retraining with a substantial labeled dataset such as Paraphrase Database (Ganitkevitch et al., 2013). The current paper goes further, showing that the following completely unsuper-vised sentence embedding is a formidable baseline: Use word embeddings com-puted using one of the popular methods on unlabeled corpus like Wikipedia, rep-resent the sentence by a weighted average of the word vectors, and then modify them a bit using PCA/SVD. This weighting improves performance by about 10{\%} to 30{\%} in textual similarity tasks, and beats sophisticated supervised methods in-cluding RNN's and LSTM's. It even improves Wieting et al.'s embeddings. This simple method should be used as the baseline to beat in future, especially when labeled training data is scarce or nonexistent. The paper also gives a theoretical explanation of the success of the above unsu-pervised method using a latent variable generative model for sentences, which is a simple extension of the model in Arora et al. (TACL'16) with new " smoothing " terms that allow for words occurring out of context, as well as high probabilities for words like and, not in all contexts.},
author = {Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
doi = {10.1016/B978-0-12-401688-0.00001-X},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/f4c13d80575f6dae60bbebeb16d550de15f64638.pdf:pdf},
isbn = {0167-2738},
issn = {0022-1899},
journal = {ICLR},
mendeley-groups = {Visual patent exploration},
pages = {1--16},
title = {{A Simple but Tough-to-Beat Baseline for Sentence Embeddings}},
year = {2017}
}

@article{Correa2017,
abstract = {This paper describes our multi-view ensemble approach to SemEval-2017 Task 4 on Sentiment Analysis in Twitter, specifically, the Message Polarity Classification subtask for English (subtask A). Our system is a voting ensemble, where each base classifier is trained in a different feature space. The first space is a bag-of-words model and has a Linear SVM as base classifier. The second and third spaces are two different strategies of combining word embeddings to represent sentences and use a Linear SVM and a Logistic Regressor as base classifiers. The proposed system was ranked 18th out of 38 systems considering F1 score and 20th considering recall.},
archivePrefix = {arXiv},
arxivId = {1704.02263},
author = {Corr{\^{e}}a, Edilson A. and Marinho, Vanessa Queiroz and dos Santos, Leandro Borges},
eprint = {1704.02263},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/S17-2100.pdf:pdf},
pages = {611--615},
title = {{NILC-USP at SemEval-2017 Task 4: A Multi-view Ensemble for Twitter Sentiment Analysis}},
url = {http://arxiv.org/abs/1704.02263},
year = {2017}
}

@article{OConnell2006,
abstract = {"The book provides a comprehensive treatment of multidimensional scaling (MDS), a family of statistical techniques for analyzing the structure of (dis)similarity data. Such data are widespread, including, for example, intercorrelations of survey items, direct ratings on the similarity on choice objects, or trade indices for a set of countries. MDS represents the data as distances among points in a geometric space of low dimensionality. This map can help to see patterns in the data that are not obvious from the data matrices. MDS is also used as a psychological model for judgments of similarity and preference." "This book may be used as an introduction to MDS for students in psychology, sociology, and marketing. The prerequisite is an elementary background in statistics. The book is also well suited for a variety of advanced courses on MDS topics. All the mathematics required for more advanced topics is developed systematically."--Jacket.},
author = {O'Connell, Ann A. and Borg, Ingwer. and Groenen, Patrick},
doi = {10.2307/2669710},
isbn = {9780387289816},
issn = {01621459},
journal = {Journal of the American Statistical Association},
number = {445},
pages = {338},
publisher = {Springer},
title = {{Modern Multidimensional Scaling: Theory and Applications}},
volume = {94},
year = {2006}
}

@article{Tenenbaum2000,
abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
author = {Tenenbaum, J. B. and {De Silva}, V. and Langford, J. C.},
doi = {10.1126/science.290.5500.2319},
issn = {00368075},
journal = {Science},
month = {dec},
number = {5500},
pages = {2319--2323},
pmid = {11125149},
publisher = {American Association for the Advancement of Science},
title = {{A global geometric framework for nonlinear dimensionality reduction}},
url = {https://science.sciencemag.org/content/290/5500/2319.full},
volume = {290},
year = {2000}
}

@article{hotelling1933analysis,
  title={Analysis of a complex of statistical variables into principal components.},
  author={Hotelling, Harold},
  journal={Journal of educational psychology},
  volume={24},
  number={6},
  pages={417},
  year={1933},
  publisher={Warwick \& York}
}

@article{McInnes2018,
abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
archivePrefix = {arXiv},
arxivId = {1802.03426},
author = {McInnes, Leland and Healy, John and Melville, James},
eprint = {1802.03426},
month = {feb},
title = {{UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}},
url = {http://arxiv.org/abs/1802.03426},
year = {2018}
}

@misc{Bock,
author = {Bock, Tim},
booktitle = {Displayr},
title = {{What is a Dendrogram? How to use Dendrograms | Displayr}},
url = {https://www.displayr.com/what-is-dendrogram/},
urldate = {2019-05-21}
}

@book{mcnaught1997compendium,
  title={Compendium of chemical terminology},
  author={McNaught, Alan D and Wilkinson, A},
  volume={1669},
  year={1997},
  publisher={Blackwell Science Oxford},
  doi={10.1351/goldbook.S06082}
}

@article{Liu2010,
abstract = {This paper explores several unsupervised approaches to automatic keyword extraction using meeting transcripts. In the TFIDF (term frequency, inverse document frequency) weighting framework, we incorporated part-of-speech (POS) information, word clustering, ...},
author = {Liu, Feifan and Pennell, Deana and Liu, Fei and Liu, Yang},
doi = {10.3115/1620754.1620845},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/N09-1070.pdf:pdf},
keywords = {a need to automatically,generate keywords for the,in the transcriptions,large amount of written,or spoken documents avail-,there is,therefore},
number = {June},
pages = {620},
title = {{Unsupervised approaches for automatic keyword extraction using meeting transcripts}},
year = {2010}
}

@article{Kim2009,
abstract = {Domain-specific terms provide vital semantic information for many natural language processing (NLP) tasks and applications, but remain a largely untapped resource in the field. In this paper, we propose an unsupervised method to extract domain-specific terms from the Reuters document collection using term frequency and inverse document frequency.},
author = {Kim, Su Nam and Baldwin, Timothy and Kan, Min-yen},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/U09-1013.pdf:pdf},
journal = {Proceedings of the Australasian Language Technology Association Workshop},
number = {2},
pages = {94--98},
title = {{An Unsupervised Approach to Domain-Specific Term Extraction}},
year = {2009}
}

@inbook{rose,
author = {Rose, Stuart and Engel, Dave and Cramer, Nick and Cowley, Wendy},
publisher = {John Wiley \& Sons, Ltd},
isbn = {9780470689646},
title = {Automatic Keyword Extraction from Individual Documents},
booktitle = {Text Mining},
chapter = {1},
pages = {1-20},
doi = {10.1002/9780470689646.ch1},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470689646.ch1},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470689646.ch1},
year = {2010},
keywords = {benchmark evaluation, information retrieval (IR) systems, rapid automatic keyword extraction (RAKE), stoplist generation},
abstract = {Summary Keywords are widely used to define queries within information retrieval (IR) systems as they are easy to define, revise, remember, and share. This chapter describes the rapid automatic keyword extraction (RAKE), an unsupervised, domain-independent, and language-independent method for extracting keywords from individual documents. It provides details of the algorithm and its configuration parameters, and present results on a benchmark dataset of technical abstracts, showing that RAKE is more computationally efficient than TextRank while achieving higher precision and comparable recall scores. The chapter then describes a novel method for generating stoplists, which is used to configure RAKE for specific domains and corpora. Finally, it applies RAKE to a corpus of news articles and defines metrics for evaluating the exclusivity, essentiality, and generality of extracted keywords, enabling a system to identify keywords that are essential or general to documents in the absence of manual annotations. Controlled Vocabulary Terms benchmark polls}
}

@inproceedings{Mihalcea2004,
abstract = {In this paper, we introduce TextRank a graph-based ranking model for text processing, and showhowthis model can be successfully used in natural language applications. In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.},
author = {Mihalcea, Rada and Tarau, Paul},
booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP), 2004, Barcelona, Spain},
doi = {10.1016/0305-0491(73)90144-2},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/mihalcea.emnlp04.pdf:pdf},
issn = {03050491},
number = {4},
title = {{TextRank: Bringing Order into Texts}},
volume = {45},
year = {2004}
}

@misc{Rauer2011,
author = {Rauer, Matthias},
booktitle = {Seibert Media},
title = {{Quantitative Usablility-Analysen mit der System Usability Scale (SUS) - Nachrichten, Tipps {\&} Anleitungen f{\"{u}}r Agile, Entwicklung, Atlassian-Software (JIRA, Confluence, Bitbucket, ...) und Google Cloud}},
url = {https://blog.seibert-media.net/blog/2011/04/11/usablility-analysen-system-usability-scale-sus/},
urldate = {2019-05-21},
year = {2011}
}

@article{Kim,
author = {Kim, Jaeyoung and Yoon, Janghyeok and Park, Eunjeong and Choi, Sungchul},
file = {:C$\backslash$:/Users/tskripnikova/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - Unknown - Patent Document Clustering Using Deep Embeddings.pdf:pdf},
keywords = {00-01,2010 msc,99-00,deep learning,embedding,learning,patent clustering,text mining,unsupervised},
mendeley-groups = {Visual patent exploration},
title = {{Patent Document Clustering Using Deep Embeddings}}
}

@article{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
eprint = {1802.05365},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/1802.05365.pdf:pdf},
title = {{Deep contextualized word representations}},
url = {http://arxiv.org/abs/1802.05365},
year = {2018}
}

@article{Devlin2018,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4{\%} (7.6{\%} absolute improvement), MultiNLI accuracy to 86.7 (5.6{\%} absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5{\%} absolute improvement), outperforming human performance by 2.0{\%}.},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
eprint = {1810.04805},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/1810.04805.pdf:pdf},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://arxiv.org/abs/1810.04805},
year = {2018}
}

@article{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Quoc V. and Mikolov, Tomas},
eprint = {1405.4053},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/paragraph{\_}vector.pdf:pdf},
mendeley-groups = {Visual patent exploration},
month = {may},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
year = {2014}
}

@article{Federico2017,
abstract = {The increasingly large number of available writings describing technical and scientific progress, calls for advanced analytic tools for their efficient analysis. This is true for many application scenarios in science and industry and for different types of writings, comprising patents and scientific articles. Despite important differences between patents and scientific articles, both have a variety of common characteristics that lead to similar search and analysis tasks. However, the analysis and visualization of these documents is not a trivial task due to the complexity of the documents as well as the large number of possible relations between their multivariate attributes. In this survey, we review interactive analysis and visualization approaches of patents and scientific articles, ranging from exploration tools to sophisticated mining methods. In a bottom-up approach, we categorize them according to two aspects: (a) data type (text, citations, authors, metadata, and combinations thereof), and (b) task (finding and comparing single entities, seeking elementary relations, finding complex patterns, and in particular temporal patterns, and investigating connections between multiple behaviours). Finally, we identify challenges and research directions in this area that ask for future investigations.},
author = {Federico, Paolo and Heimerl, Florian and Koch, Steffen and Miksch, Silvia},
doi = {10.1109/TVCG.2016.2610422},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/PubDat{\_}251231.pdf:pdf},
isbn = {0301-6226},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Visualization,documents,patents,scientific literature,survey},
mendeley-groups = {Visual patent exploration},
number = {9},
pages = {2179--2198},
title = {{A Survey on Visual Approaches for Analyzing Scientific Literature and Patents}},
volume = {23},
year = {2017}
}

@article{Dou2011,
abstract = {Scalable and effective analysis of large text corpora remains a challenging problem as our ability to collect textual data continues to increase at an exponential rate. To help users make sense of large text corpora, we present a novel visual analytics system, Parallel-Topics, which integrates a state-of-the-art probabilistic topic model Latent Dirichlet Allocation (LDA) with interactive visualization. To describe a corpus of documents, ParallelTopics first extracts a set of semantically meaningful topics using LDA. Unlike most traditional clustering techniques in which a document is assigned to a specific cluster, the LDA model accounts for different topical aspects of each individual document. This permits effective full text analysis of larger documents that may contain multiple topics. To highlight this property of the model, ParallelTopics utilizes the parallel coordinate metaphor to present the probabilistic distribution of a document across topics. Such representation allows the users to discover single-topic vs. multi-topic documents and the relative importance of each topic to a document of interest. In addition, since most text corpora are inherently temporal, ParallelTopics also depicts the topic evolution over time. We have applied ParallelTopics to exploring and analyzing several text corpora, including the scientific proposals awarded by the National Science Foundation and the publications in the VAST community over the years. To demonstrate the efficacy of ParallelTopics, we conducted several expert evaluations, the results of which are reported in this paper.},
author = {Dou, Wenwen and Wang, Xiaoyu and Chang, Remco and Ribarsky, William},
doi = {10.1109/VAST.2011.6102461},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/dou2011parallel.pdf:pdf},
isbn = {9781467300131},
journal = {VAST 2011 - IEEE Conference on Visual Analytics Science and Technology 2011, Proceedings},
keywords = {H.5.2 [INFORMATION INTERFACES AND PRESENTATION]: U},
mendeley-groups = {Visual patent exploration},
pages = {231--240},
title = {{ParallelTopics: A probabilistic approach to exploring document collections}},
year = {2011}
}

@article{Jiang2016,
abstract = {Cross-domain research topic mining can help users find relationships among related research domains and obtain a quick overview of these domains. This study investigates the evolution of cross-domain topics of three interdisciplinary research domains and uses a visual analytic approach to determine unique topics for each domain. This study also focuses on topic evolution over 10 years and on individual topics of cross domains. A hierarchical topic model is adopted to extract topics of three different domains and to correlate the extracted topics. A simple yet effective visualization interface is then designed, and certain interaction operations are provided to help users more deeply understand the visualization development trend and the correlation among the three domains. Finally, a case study is conducted to demonstrate the effectiveness of the proposed method. Graphical Abstract},
author = {Jiang, Xinyi and Zhang, Jiawan},
doi = {10.1007/s12650-015-0323-9},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/Xinyi.pdf:pdf},
isbn = {10.1007/s12650-015-0323-9},
issn = {18758975},
journal = {Journal of Visualization},
keywords = {Text visualization,Topic mining,Visual analysis},
mendeley-groups = {Visual patent exploration},
number = {3},
pages = {561--576},
publisher = {Springer Berlin Heidelberg},
title = {{A text visualization method for cross-domain research topic mining}},
volume = {19},
year = {2016}
}

@misc{Bostock2019a,
author = {Bostock, Mike},
title = {{Scatterplot Matrix Brushing - bl.ocks.org}},
url = {https://bl.ocks.org/mbostock/4063663},
urldate = {2019-05-24},
year = {2019}
}

@article{Skupin2002,
abstract = {A cartographic approach to mapping nongeographic information helps to manage graphic complexity in visualizations. It aids domain comprehension by forcing us to use the same cognitive skills we use when viewing geographic maps. The author presents a distinctly cartographic approach to mapping nongeographic information. Focusing on the text content of a set of conference abstracts, we can derive 2D visualizations of information spaces that address complexity and automation},
author = {Skupin, Andr{\'{e}}},
doi = {10.1109/38.974518},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/00974518.pdf:pdf},
issn = {02721716},
journal = {IEEE Computer Graphics and Applications},
mendeley-groups = {Visual patent exploration},
number = {1},
pages = {50--58},
publisher = {IEEE},
title = {{A Cartographic Approach to Visualizing Conference Abstracts}},
volume = {22},
year = {2002}
}

@article{Skupin2004a,
abstract = {From an informed critique of existing methods to the development of original tools, cartographic engagement can provide a unique perspective on knowledge domain visualization. Along with a discussion of some principles underlying a cartographically informed visualization methodology, results of experiments involving several thousand conference abstracts will be sketched and their plausibility reflected on.},
annote = {Hierarchical clusters with self-organizing maps},
author = {Skupin, A.},
doi = {10.1073/pnas.0307654100},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/5274.full.pdf:pdf},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
mendeley-groups = {Visual patent exploration},
number = {Supplement 1},
pages = {5274--5278},
pmid = {14764896},
title = {{The world of geography: Visualizing a knowledge domain with cartographic means}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0307654100},
volume = {101},
year = {2004}
}

@ARTICLE{utopian, 
author={J. {Choo} and C. {Lee} and C. K. {Reddy} and H. {Park}}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={UTOPIAN: User-Driven Topic Modeling Based on Interactive Nonnegative Matrix Factorization}, 
year={2013}, 
volume={19}, 
number={12}, 
pages={1992-2001}, 
keywords={data analysis;data visualisation;interactive systems;matrix decomposition;text analysis;real-world document corpuses;user-driven manner;topic modeling method;semisupervised formulation;reliable visual analytics system;flexible visual analytics system;user feedback;visual text analytics;latent Dirichlet allocation;probabilistic graphical modeling;topic modeling techniques;text document collection analysis;user-driven topic modeling based on interactive nonnegative matrix factorization;UTOPIAN;Analytical models;Visual analytics;Computational modeling;Interactive states;Context modeling;Analytical models;Visual analytics;Computational modeling;Interactive states;Context modeling;text analytics;Latent dirichlet allocation;nonnegative matrix factorization;topic modeling;visual analytics;interactive clustering;Artificial Intelligence;Computer Graphics;Computer Simulation;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Models, Statistical;Natural Language Processing;Pattern Recognition, Automated;Software}, 
doi={10.1109/TVCG.2013.212}, 
ISSN={1077-2626}, 
month={Dec},}

@article{Herr2014a,
author = {Herr, Dominik and Han, Qi and Lohmann, Steffen and Br??gmann, S??ren and Ertl, Thomas},
doi = {10.5121/ijcis.2012.2406},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/ipamin2014{\_}paper8.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {CPC,Classification,IPC,Mining,Patents,Retrieval,Tag cloud,Visual analysis,Visualization},
mendeley-groups = {Visual patent exploration},
number = {IPaMin},
title = {{Visual exploration of patent collections with IPC clouds}},
volume = {1292},
year = {2014}
}

@inproceedings{Giereth2007,
abstract = {Patents are an invaluable source of scientific and technological information. Due to the strongly increasing number of patent applications and the broadening of the objectives of patent analysis, there is a great demand for ubiquitous access to patent information and for flexible visualizations meeting the requirements of different groups of users. In this paper we propose new visualization techniques for patent information and approaches for interactively exploring this information in web based environments. We show how these visualizations can be integrated into existing web portals by using a new paradigm that we call Semantic Lens.},
author = {Giereth, Mark and Koch, Steffen and Rotard, Martin and Ertl, Thomas},
booktitle = {Proceedings of the International Conference on Information Visualisation},
doi = {10.1109/IV.2007.141},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/Web Based Visual Exploration of Patent Information.pdf:pdf},
isbn = {0-7695-2900-3},
issn = {10939547},
mendeley-groups = {Visual patent exploration},
title = {{Web based visual exploration of patent information}},
year = {2007}
}

@article{Garfield2004,
abstract = {To better understand the topic of this colloquium, we have created a series of databases related to knowledge domains (dynamic systems [small world/Milgram], information visualization [Tufte], co-citation [Small], bibliographic coupling [Kessler], and scientometrics [Scientometrics]). I have used a software package called HistCite(TM) which generates chronological maps of subject (topical) collections resulting from searches of the ISI Web of Science(R) or ISI citation indexes (SCI, SSCI, and/or AHCI) on CD-ROM. When a marked list is created on WoS, an export file is created which contains all cited references for each source document captured. These bibliographic collections, saved as ASCII files, are processed by HistCite in order to generate chronological and other tables as well as historiographs which highlight the most-cited works in and outside the collection. HistCite also includes a module for detecting and editing errors or variations in cited references as well as a vocabulary analyzer which generates both ranked word lists and word pairs used in the collection. Ideally the system will be used to help the searcher quickly identify the most significant work on a topic and trace its year-by-year historical development. In addition to the collections mentioned above, historiographs based on collections of papers that cite the Watson-Crick 1953 classic paper identifying the helical structure of DNA were created. Both year-by-year as well as month-by-month displays of papers from 1953 to 1958 were necessary to highlight the publication activity of those years.},
author = {Garfield, Eugene},
doi = {10.1177/0165551504042802},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/jis30(2)p119-145y2004.pdf:pdf},
isbn = {0165-5515},
issn = {01655515},
journal = {Journal of Information Science},
keywords = {Citation analysis,DNA structure,HistCite,Historiography,Information visualization,Knowledge domains,Mapping,Small world concept,Software},
mendeley-groups = {Visual patent exploration},
number = {2},
pages = {119--145},
title = {{Historiographic mapping of knowledge domains literature}},
volume = {30},
year = {2004}
}

@article{Chen2004,
abstract = {This article introduces a previously undescribed method progressively visualizing the evolution of a knowledge domain's cocitation network. The method first derives a sequence of cocitation networks from a series of equal-length time interval slices. These time-registered networks are merged and visualized in a panoramic view in such a way that intellectually significant articles can be identified based on their visually salient features. The method is applied to a cocitation study of the superstring field in theoretical physics. The study focuses on the search of articles that triggered two superstring revolutions. Visually salient nodes in the panoramic view are identified, and the nature of their intellectual contributions is validated by leading scientists in the field. The analysis has demonstrated that a search for intellectual turning points can be narrowed down to visually salient nodes in the visualized network. The method provides a promising way to simplify otherwise cognitively demanding tasks to a search for landmarks, pivots, and hubs.},
author = {Chen, C.},
doi = {10.1073/pnas.0307513100},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/pnas2004.pdf:pdf},
isbn = {0027-8424 (Print)$\backslash$n0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
mendeley-groups = {Visual patent exploration},
number = {Supplement 1},
pages = {5303--5310},
pmid = {14724295},
title = {{Searching for intellectual turning points: Progressive knowledge domain visualization}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0307513100},
volume = {101},
year = {2004}
}

@article{Zhao2013,
abstract = {Many datasets, such as scientific literature collections, contain multiple heterogeneous facets which derive implicit relations, as well as explicit relational references between data items. The exploration of this data is challenging not only because of large data scales but also the complexity of resource structures and semantics. In this paper, we present PivotSlice, an interactive visualization technique which provides efficient faceted browsing as well as flexible capabilities to discover data relationships. With the metaphor of direct manipulation, PivotSlice allows the user to visually and logically construct a series of dynamic queries over the data, based on a multi-focus and multi-scale tabular view that subdivides the entire dataset into several meaningful parts with customized semantics. PivotSlice further facilitates the visual exploration and sensemaking process through features including live search and integration of online data, graphical interaction histories and smoothly animated visual state transitions. We evaluated PivotSlice through a qualitative lab study with university researchers and report the findings from our observations and interviews. We also demonstrate the effectiveness of PivotSlice using a scenario of exploring a repository of information visualization literature.},
author = {Zhao, Jian and Collins, Christopher and Chevalier, Fanny and Balakrishnan, Ravin},
doi = {10.1109/TVCG.2013.167},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/06634163.pdf:pdf},
isbn = {1077-2626 VO  - 19},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Faceted browsing,dynamic query,information visualization,interaction,network exploration,visual analytics},
mendeley-groups = {Visual patent exploration},
number = {12},
pages = {2080--2089},
pmid = {24051774},
title = {{Interactive exploration of implicit and explicit relations in faceted datasets}},
volume = {19},
year = {2013}
}

@ARTICLE{abello, 
author={J. {Abello} and S. {Hadlak} and H. {Schumann} and H. {Schulz}}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={A Modular Degree-of-Interest Specification for the Visual Analysis of Large Dynamic Networks}, 
year={2014}, 
volume={20}, 
number={3}, 
pages={337-350}, 
keywords={formal specification;graph theory;modular degree-of-interest specification;visual analysis;large dynamic networks;static networks;local network elements;structure information;numerical attributes;temporal evolution;tailored visualization;DoI specification complements;dynamic network;interactive definition;Visualization;Context;Data visualization;Radiation detectors;Educational institutions;Electronic mail;Navigation;Time-varying graphs;dynamic graph visualization;degree-of-interest}, 
doi={10.1109/TVCG.2013.109}, 
ISSN={1077-2626}, 
month={March},}

@INPROCEEDINGS{nakazawa, 
author={R. {Nakazawa} and T. {Itoh} and T. {Saito}}, 
booktitle={2015 19th International Conference on Information Visualisation}, 
title={A Visualization of Research Papers Based on the Topics and Citation Network}, 
year={2015}, 
volume={}, 
number={}, 
pages={283-289}, 
keywords={citation analysis;data visualisation;pattern clustering;citation network;Google Scholar;citation visualization technique;topic-based paper clustering;LDA;latent Dirichlet allocation;clustered network;Visualization;Skin;Hardware;Graphics processing units;Lighting;Color;Image processing;Citation network visualization;edge bundling;topic-based clustering}, 
doi={10.1109/iV.2015.58}, 
ISSN={1550-6037}, 
month={July},}

@article{gretarsson,
author = {Gretarsson, Brynjar and O'Donovan, John and Bostandjiev, Svetlin and Höllerer, Tobias and Asuncion, Arthur and Newman, David and Smyth, Padhraic},
year = {2012},
month = {02},
pages = {},
title = {TopicNets: Visual Analysis of Large Text Corpora with Topic Modeling},
volume = {3},
journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},
doi = {10.1145/2089094.2089099}
}

@book{Ruotsalainen2008,
abstract = {Approximately 80{\%} of scientific and technical information can be found from patent documents alone, according to a study carried out by the European Patent Office. Patents are also a unique source of information since they are collected, screened and published according to internationally agreed standards. In addition to being an extremely valuable source of technology intelligence, patent documents offer a business competitive intelligence by revealing a competitor's strengths and strategies. Information gained from patents can also help in locating partners for cross-licensing and collaboration. Since the patent system was established, more than 60 million patent applications have been published. It would be impossible to find and analyze relevant documents manually. The need for analysis and evaluation tools for patents has been acknowledged by many solution providers. New solutions are continuously coming onto the market; tools for reading and evaluating individual patents and tools for analyzing sets of patent documents. Solutions of the latter type can still be roughly divided into two groups: tools for retrieving and preparing basic statistics for patent documents, and tools for visualization and progressive analysis of patents. The former group deals only with data in a structured form, whereas the latter also analyzes unstructured text and other data. In this study, four efficient tools for analyzing patent documents were tested: Thomson Reuter's Aureka and Thomson Data Analyzer, Biowisdom's OmniViz, and STN's STN AnaVist. All four tools analyze structured and unstructured data alike. They all visualize the results achieved from clustering the text fields of patent documents and either provide basic statistics graphs themselves or contain filters for performing them with other solutions. The tools were tested with two cases, evaluating their ability to offer technology and business intelligence from patent documents for companies' daily business. Being aware of the state of the art of relevant technology areas is crucial for a company's innovation process. Knowledge of developed techniques and products forestalls overlapping R{\&}D projects and thereby prevents unnecessary investment. Equally important is the recognition of other actors operating in the field. Benchmarking and evaluating a competitor's R{\&}D and market strategies aids in managing one's own processes and locating possible parties for collaboration or cross-licensing. Copyright {\textcopyright} VTT 2008.},
author = {Ruotsalainen, Laura},
booktitle = {VTT Tiedotteita - Valtion Teknillinen Tutkimuskeskus},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/VTTDataMiningTools.pdf:pdf;:C$\backslash$:/Users/tskripnikova/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruotsalainen - 2008 - Data mining tools for technology and competitive intelligence.pdf:pdf},
isbn = {9789513872410},
issn = {12350605},
keywords = {Competitive intelligence,Data mining,Patent data,Patent mapping,Patent mining,Technology intelligence,Text mining,Visualization},
mendeley-groups = {Existing patent {\&} paper visualizations},
number = {2451},
pages = {1--64},
title = {{Data mining tools for technology and competitive intelligence}},
year = {2008}
}

@ARTICLE{hetzler, 
author={E. {Hetzler} and A. {Turner}}, 
journal={IEEE Computer Graphics and Applications}, 
title={Analysis experiences using information visualization}, 
year={2004}, 
volume={24}, 
number={5}, 
pages={22-26}, 
keywords={data visualisation;data analysis;information visualization;professional analysts;text exploitation system;Information analysis;Application software;Image analysis;Demography;Engineering profession;Data visualization;Production systems;Usability;Feedback;Timing}, 
doi={10.1109/MCG.2004.22}, 
ISSN={0272-1716}, 
month={Sep.},}

@article{Boyack2002,
abstract = {* Sandia is a multiprogram laboratory operated by Sandia Corporation, a Lockheed Martin Company, for the United States Department of Energy under Contract DE-AC04-94AL85000. We present the application of our knowledge visualization tool, VxInsight {\textregistered} , to enable domain analysis for science and technology management within the enterprise. Data mining from sources of bibliographic information is used to define subsets of information relevant to a technology domain. Relationships between the individual objects (e.g. articles) are identified using citations, descriptive terms, or textual similarities. Objects are then clustered using a force-directed placement algorithm to produce a terrain view of the many thousands of objects. A variety of features which allow exploration and manipulation of the landscapes and which give detail-on-demand, enable quick and powerful analysis of the resulting landscapes. Examples of domain analyses used in S{\&}T management at Sandia are given.},
author = {Boyack, Kevin W. and Wylie, Brian N. and Davidson, George S.},
doi = {10.1002/asi.10066},
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/Boyack{\_}et{\_}al-2002-Journal{\_}of{\_}the{\_}American{\_}Society{\_}for{\_}Information{\_}Science{\_}and{\_}Technology.pdf:pdf},
isbn = {1532-2890},
issn = {15322882},
journal = {Journal of the American Society for Information Science and Technology},
mendeley-groups = {Visual patent exploration},
title = {{Domain visualization using VxInsight{\textregistered} for science and technology management}},
year = {2002}
}

@techreport{graphene,
file = {:C$\backslash$:/Users/tskripnikova/Google Drive/Thesis/Literature/Graphene{\_}-{\_}the{\_}worldwide{\_}patent{\_}landscape{\_}in{\_}2015.pdf:pdf},
institution = {UK Intellectual Property Office Infromatics Team},
keywords = {Graphene: The worldwide patent landscape 2015,res},
title = {{Graphene: The worldwide patent landscape in 2015}},
url = {https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment{\_}data/file/470918/Graphene{\_}-{\_}the{\_}worldwide{\_}patent{\_}landscape{\_}in{\_}2015.pdf},
year = {2015}
}

